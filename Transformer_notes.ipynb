{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_notes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FpnkM3TCJWlY",
        "9OFydIJqolUs",
        "K8oAcLPVm9F0"
      ],
      "authorship_tag": "ABX9TyOw2YKxq2Eb9TFnHGjThHW5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvdheram/Social-bias-Detection/blob/main/Transformer_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xla2sm6TEwWH"
      },
      "source": [
        "# RNN seq2seq model with Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_8d0uoeFUCV"
      },
      "source": [
        "Resources : \r\n",
        "\r\n",
        "\r\n",
        "*  Seq2seq with attention : https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwPB-2Gzpkwi"
      },
      "source": [
        "## RNN sequence-to-sequence models  :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2ATBS9zE437"
      },
      "source": [
        "Why and What RNN?\r\n",
        "\r\n",
        "* Usefull When Dealing with Sequential data [current data depends on previous].\r\n",
        "* Regular NN with a loop (recurrent unit; hidden state passed every time step). Unrolling of RNN leads a very deep Feed forward neural network.\r\n",
        "\r\n",
        "RNN-Seq2Seq :\r\n",
        "\r\n",
        "  *   Takes sequence of items ( words, letters, features of an image ..etc) as input and ouputs a sequence of items w.r.t tasks ( machine translation, text summarization and image captionning etc..)\r\n",
        "\r\n",
        "* Elements :\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "      1. Hidden state - Previous information or context from previous inputs \r\n",
        "      2. Input Vector - Element in the sequence\r\n",
        "\r\n",
        "    Output: \r\n",
        "      3. Output Vector - Final/intermediate?? output of the sequence \r\n",
        "\r\n",
        "RNN :\r\n",
        "\r\n",
        "      for each time step (word_embedding in sequence):\r\n",
        "\r\n",
        "        Hidden state #0 + Input vector #1 => hidden state #1 + Input Vector#2.. - hidden unit (context) passed along with input to RNN => output vector #1 \r\n",
        "\r\n",
        "\r\n",
        "Problem with vanilla RNN:\r\n",
        "\r\n",
        "* Short term memory ( ability to retain information from previous wrong steps)\r\n",
        "\r\n",
        "  Why ??\r\n",
        " \r\n",
        "* Vanishing gradients due to backpropagation.   \r\n",
        "\r\n",
        "  How??\r\n",
        "\r\n",
        "* Weights adjusted ( reduce the loss ) of a layer in deep feed forward layer depends on the previous layers gradients. If the gradients of the previous layer is less, it effects the current layer nodes ( first layers gradients becomes very low or no learning).\r\n",
        "\r\n",
        "  Partial Solution??\r\n",
        "\r\n",
        "* Varients of RNN architecture \r\n",
        "  1. GRU ( Gated Reccurent Unit )\r\n",
        "  2. LSTM ( Long Short Term Memeory )\r\n",
        "\r\n",
        "\r\n",
        "Eg. \r\n",
        "\r\n",
        "Input Vector : Word in a sequence\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncrdJNYKGNG4"
      },
      "source": [
        "## Illustrated example: Nueral Machine traslation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okyCv7I5IV6i"
      },
      "source": [
        "Seq2seq Machine traslation:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9-aDNCUahv8"
      },
      "source": [
        "Encoder-Decoder architecture (RNN) :\r\n",
        "\r\n",
        "*   Encoder transforms input sequence into vector ( Context )\r\n",
        "*   Context (last hidden state of the sequence) sent to decoder which decodes vector sequence element by element.\r\n",
        "\r\n",
        "        Input_sequence -> Encoder -> Context -> Decoder -> Translation \r\n",
        "\r\n",
        "* Size of context vector set by the number of hidden units in the encoder RNN.\r\n",
        "* The last hidden state of the Encoder is the context passed along to the decoder to decode the sequence of words.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMSqGoofKvF9"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhgMFc9MK_x8"
      },
      "source": [
        "Why??\r\n",
        "\r\n",
        "* Dealing with long term dependencies with long sentences.\r\n",
        "* Dealing with different words (context) which contribute to single word in translation .\r\n",
        "\r\n",
        "Auto - regressive CNN's (WaveNet, ByteNet) used as a solution, but the convolution layers could capture history by positions rather than content.  \r\n",
        "\r\n",
        "What??\r\n",
        "\r\n",
        "* Allows model to focus on relavant parts (words) of input sequence with weights representing the relative importance of different words (keys) for the particular word (query) being translated. \r\n",
        "\r\n",
        "Contribution ??\r\n",
        "\r\n",
        "* Encoder :\r\n",
        "  \r\n",
        "  All the hidden states of encoder passed to decoder rather than the last hidden state of the sequence. \r\n",
        "\r\n",
        "* Decoder :\r\n",
        "\r\n",
        "  Attention scores and context vector :\r\n",
        "\r\n",
        "  for each time step:\r\n",
        "\r\n",
        "    1. Score each hidden state, associated with words in the sequence.\r\n",
        "    2. Multiply each hidden state (vector) by softmaxed (0-1) attention score. thus giving attention to words with high score.\r\n",
        "    3. Sum-up the weighted vectors which is context vector.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snwrsUWhcvS9"
      },
      "source": [
        "#### Scoring with attention  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X8xmqEcdKSL"
      },
      "source": [
        "Decoder RNN :\r\n",
        "\r\n",
        "Time step #1 :\r\n",
        "\r\n",
        "1. Takes the `<END>` embedding as input vector and initial decoder hidden state to feed into the RNN unit.   \r\n",
        "2. Decoder produces output and hidden state vector (h4); output vector discarded.\r\n",
        "3. Scoring: \r\n",
        "  1. Use encoder hidden states (h1,h2,h3) and  h4 hidden state vector of decoder to calcualte attention scores and thus form context vector(c4). \r\n",
        "4. Concatenate h4 and c4 into single vector.\r\n",
        "5. Pass through feedforward neural network \r\n",
        "6. Output vector (O1) of feedforward neural network is the translated word for time step#1.\r\n",
        "7. Hidden unit (h4) and output vector (O1) of previous step passed as input to the next time step #2 and repeated. \r\n",
        "\r\n",
        "\r\n",
        "Advantages:\r\n",
        "\r\n",
        "1. With attention and scoring the text alignment is learned while training. \r\n",
        "\r\n",
        "Eg. French to English \r\n",
        "\r\n",
        "French : je suis etudiant\r\n",
        "\r\n",
        "1. French Encoder hidden state vector (h) [h1,h2,h3]\r\n",
        "  2. Decoder hidden state vector step #1 [1,0,0] - I \r\n",
        "  3. Decoder hidden state vector step #2 [0,1,0] - am \r\n",
        "  4. Decoder hidden state vector step #3 [0,0.5,0.5] - a \r\n",
        "  5. Decoder hidden state vector step #4 [0,0,1] - Student\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jBuhiOKlGC9"
      },
      "source": [
        "# Transformers \r\n",
        "\r\n",
        "Transformers Paper : https://arxiv.org/abs/1706.03762\r\n",
        "\r\n",
        "Lecture video : https://www.youtube.com/watch?v=OyFJWRnt_AY&ab_channel=PascalPoupart\r\n",
        "\r\n",
        "Annotated Transformer : http://nlp.seas.harvard.edu/2018/04/03/attention.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ5KlRt3Fq1Y"
      },
      "source": [
        "Transformers : A model that uses attention to boost speed of training. \r\n",
        "\r\n",
        "Contributions as compared to RNNs :\r\n",
        "\r\n",
        "* Facilitate long range dependencies with attention mechanism\r\n",
        "* Avoid gradient vanishing and explosion as transformers does computation for **entire sequence simultaneously** rather than linearly done in the RNN.\r\n",
        "* Fewer training steps due to whole sepquence being processed rather than processing each word linearly in each time step.\r\n",
        "* No recurrence ( sequential computation like RNN) which facilitates parallel computation (GPU)\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVRtB4QBfiQU"
      },
      "source": [
        "## Attention Mechanism in Nueral architecture "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of5eu1SxI70j"
      },
      "source": [
        "### Transformer Nueral Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBReIHzsJEMJ"
      },
      "source": [
        "Building blocks of Transformer - Illustrated with machine translation :\r\n",
        "\r\n",
        "    Input : Word embeddings; vector_size - 512; (bottom-most encoder)\r\n",
        "\r\n",
        "    output : vector (floats) for the words in the sequence -> Fully connected layer -> logits vector ( based on vocab size learned during training) -> softmax layer (all positive, all add up to 1.0) -> argMax(softmax layer)  -> Word\r\n",
        "\r\n",
        "**Iteration w.r.t a single word** : \r\n",
        "\r\n",
        "1.  Encoders - stack\r\n",
        "\r\n",
        "        Input : Input word - embedding (Eng)\r\n",
        "        Output : Encoded input word - embedding with context ( information of other relavant words attended throughout the stack )\r\n",
        "\r\n",
        "  1. Attention ( self / multi-head ) \r\n",
        "  2. Add & Layer - normalization \r\n",
        "  3. Feed forward neural network \r\n",
        "\r\n",
        "2.   Decoders - stack\r\n",
        "\r\n",
        "          Input : Encoded input Word - embedding (Eng) and masked output word embedding(Ger; for having information of previous words until the predicting word and mask future words)\r\n",
        "          Output : Probabilitiy distribution over the word in the dictionary \r\n",
        "\r\n",
        "  1. Attention ( self / multi-head ) \r\n",
        "  2. Add & Layer - normalization \r\n",
        "  3. Encoder - Decoder attention \r\n",
        "  4. Feed forward neural network \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtKioZKNajN1"
      },
      "source": [
        "#### Encoder - stack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrOh4s4dxo1T"
      },
      "source": [
        "High level description of steps in encoder stack:\r\n",
        "\r\n",
        "1.  Inputs : Entire sequence of words as input.\r\n",
        "2.  Input  Embedding : Word embedding of input.\r\n",
        "3.  Positional encoding : Add positional encoding (vector) for words in the sequence which provide meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.\r\n",
        "4.  Multi-head attention : Multi-head attention used to compute attention of each word in the sequence vector againt other words with multiple heads (multiple combination of contributing words). \r\n",
        "  * Idea: Treat each word as query, find keys (other words) in the sentence based on similarity and take the dot product of query vector and key vector to compute attention score of each word against other words. \r\n",
        "\r\n",
        "  * Having several stacks of encoders (attention) leads to finding attention of not only single words but of pairs; pair of pairs .. \r\n",
        "\r\n",
        "  * Multi-head attention leads to finding multiple attention ( multiple weighted combination of other words which contribute)  w.r.t the query word.\r\n",
        "5. Add & Norm : Add the original input to the output of multi-head attention and normalizing the layer ( mean - 0 and variance - 1).\r\n",
        "        hidden-unit(h) = g (h - mean (h) ) / standard-deviation(h)\r\n",
        "          g - variable \r\n",
        "          \r\n",
        "        Why ??\r\n",
        "          covariate- shift ( gradient dependencies between layers)  \r\n",
        "\r\n",
        "6. Feed Forward layer : A layer with no cycles/ recurrence and used to process or fit output from one layer to next.\r\n",
        "7. Add & Norm \r\n",
        "\r\n",
        "Key points:\r\n",
        "\r\n",
        "*  All Encoders receive vectors of size 512. \r\n",
        "  * Bottom_most : word bedding vector\r\n",
        "  * Others : output of the previous layer encoder.\r\n",
        "*  Words in each encoder flow through its own path (Key factor for parellel execution with GPU) and hence entire sequence of words is fed as input.\r\n",
        "  * Dependencies exists between words in the \"attention layer\".\r\n",
        "* Pipeline for a word in sequence :  \r\n",
        "\r\n",
        "      Input ( single Word ) -> embedding -> ( Query, key, value ) -> attention score against other words in sequence (q*k) -> normalize (dimention_size) -> softmax (0-1) -> softmax * value  -> summation -> encoding of word with context.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2pdUsv7y669"
      },
      "source": [
        "#### Decoder-stack "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlKTTDWXzh9_"
      },
      "source": [
        "Input: Multiple stacks of key,value vector ( multi-head attention) from encoder and output embeddings (translation) from decoder after first iteration.\r\n",
        "\r\n",
        "Output : Probability distribution over the vocab for each position of word. (translation)\r\n",
        "\r\n",
        "\r\n",
        "High level description of steps in decoder stack:\r\n",
        "\r\n",
        "1. Output Embedding : word embedding of outputs\r\n",
        "2. Positional encoding : Adding positional encoding to decoders input to indicate position of each words.\r\n",
        "3. Masked Multi-head attention :  Mask (-infinity) the positions of the future words, for the self attention to focus on earlier positions (words) in the output rather than the future positions while training. \r\n",
        "4. Add and norm : Add the residuals and normlize the the output of the multi-head attention.\r\n",
        "5. Multi-head attention : Takes the input from the encoder (multiple key, value)  and output (masked attention scores) from the masked multi-head attention to calculate the inter-attention between the input and output ( generated untill that point).\r\n",
        "6. Feed forward layer ; add and norm : Adjust the dimentionality and do a layer normalization ( mean - 0 and variance - 1). \r\n",
        "7. Linear layer : Project the attention vector into the logits vector with dimentionality size of the vocab (unique words) learned during the training. \r\n",
        "8. Softmax layer : Turns the scores into probabilities ( all positive; all add upto 1.0); the cell with highest probability choosen and word associated (vocab) with it is produced as output for the time step which again goes into decoder stack as input. \r\n",
        "\r\n",
        "\r\n",
        "Note :\r\n",
        "\r\n",
        "* The output of decoder being fed in again might seems as recurrence, but with teacher forcing technique this can be avoided.\r\n",
        "\r\n",
        "* Teacher forcing is a method used while training wherein the output of the decoder  ( previous prediction ) is assumed to be correct and the correct translation is directly fed into the decoder input rather than waiting for the output in that timestep. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzRPLyH8Dzsm"
      },
      "source": [
        "##### Loss Function "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E8pnX7yFgDF"
      },
      "source": [
        "Subtraction of the model output vector ( probability distribution over vocab ) and the desired output vector ( probability distribution over vocab ) for every position in the sequence of words until the `<'end of sentence'>` is reached. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAOcpx4SI_5I"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QtjM-K8GeyK"
      },
      "source": [
        "Attention Mechanism \r\n",
        "\r\n",
        "* **Mimics the retrieval of a value `v` for a query `q` based on a key `k` in the database**.\r\n",
        "\r\n",
        "Elements:\r\n",
        "\r\n",
        "* Query vector - q\r\n",
        "* Key vectors -  k\r\n",
        "* Value vectors - v\r\n",
        "\r\n",
        "      attention(q,k,v) = Sum( (similarity(q,k)) * v) \r\n",
        "\r\n",
        "Steps :\r\n",
        "\r\n",
        "1. Compute similarity Measure f(Query, keys):\r\n",
        "\r\n",
        "        Input : Vectors ( Query, keys)\r\n",
        "        Output : similarity measure (Scalar) \r\n",
        "  options :\r\n",
        "  1. Dot product of (q,k)   \r\n",
        "  2. Scaled dot product (q,k)/ sqrt(dimentionality of each key) \r\n",
        "  3. General dot product(q * w  k); w(weights) - Query vector projected to embedding space for similarity measure (multi-head).\r\n",
        "  \r\n",
        "\r\n",
        "2. Compute weights (a):\r\n",
        "\r\n",
        "        Input : similarity measure (Scalar)\r\n",
        "        Output : Weights (Vector) \r\n",
        "\r\n",
        "        Formula : Fully connected Softmax layer \r\n",
        "        a(i) = similarity_measure (i) / sum of all similarity_measure \r\n",
        "\r\n",
        "3. Weighted combination (weight, value):\r\n",
        "\r\n",
        "        Input : Weights (Vector) \r\n",
        "        Output : Attention value (vector)  \r\n",
        "\r\n",
        "        Formula : sum(weights * value of keys) \r\n",
        "        \r\n",
        "        Paper (scaled (by length) dot product attention): softmax ( weight * values) / sqrt(d) ) \r\n",
        "        d - dimentionality \r\n",
        "\r\n",
        "* Query, key, values are matrices ( formed by combining multiple (q,k,v) vectors; described in paper to speedup training for multiple queries) formed by multiplying word embedding (x) and projection matrices (WQ,WK,WV) learned during training. \r\n",
        "  * Why projection matrices?\r\n",
        "    * Reduce embedding (x)  dimention via weight transformation / projection.\r\n",
        "* keys == values in self-attention / intra seqeunce - attention. Keys and values are different when attention used to relate two different sequences.  \r\n",
        "* Key, value - Vocab learned during training.\r\n",
        "* Query - word for which attention score is being calculated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROqZO3KnCrR1"
      },
      "source": [
        "### Multi-head attention in Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBUdLE0kCwKm"
      },
      "source": [
        "Multi-head attention : Compute multiple attentions per query ( word ) with different weights (values).\r\n",
        "\r\n",
        "* Multiple sets of (Query,key,value) \r\n",
        "\r\n",
        "  * Why ??\r\n",
        "    * The weighted combination in the attention mechanism combines the attention of every word into a matrix wherein the other words might dominate the actual word.\r\n",
        "\r\n",
        "  * What ??\r\n",
        "    * Multiple sets are like multiple feature maps / filters in CNN which gives multiple contexts or \"representation subspaces\" for each word.\r\n",
        "\r\n",
        "  * How ??\r\n",
        "      * Extract different features w.r.t word by focusing on different positions and combination of different positions in sentence.\r\n",
        "\r\n",
        " Eg.  heads = 2 (q,k,v)\r\n",
        "\r\n",
        "          Sentence :  \" The animal didn't cross the street because it was too tired\".\r\n",
        "\r\n",
        "          Coreference resolution of \"it\" :  animal, tire \r\n",
        "\r\n",
        "\r\n",
        "Pipeline :\r\n",
        "\r\n",
        "    Multiple-sets( # of heads) of ( q,k,v ) via W projection matrices -> Scaled dot product attention( # of heads) -> concat (head1,head2,...headh) -> linear -> multi-head attention\r\n",
        "\r\n",
        "\r\n",
        "Masked Multi - head attention: \r\n",
        "\r\n",
        "* Multi-head where some values are masked ( i.e probabilities of masked values ( -inf) are nullified ( with softmax operation ) to prevent them from being selected ).\r\n",
        "* In decoder, the output values should depend on previous output rather than future outputs which is done through masked multi-head attention.\r\n",
        "\r\n",
        "        MaskedAttention (q,k,v) = softmax( q + M / sqrt (d) ) v\r\n",
        "\r\n",
        "        where M is a mask matrix of 0's and -inf's\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ7ysonYyXdE"
      },
      "source": [
        "# BERT : Bi-directional encoder representation from Transformers\r\n",
        "\r\n",
        "Source : http://jalammar.github.io/illustrated-word2vec/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpnkM3TCJWlY"
      },
      "source": [
        "### Word Embeddings\r\n",
        "\r\n",
        "Source : https://www.tensorflow.org/tutorials/text/word_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3Zdh3Z0ocgm"
      },
      "source": [
        "What ??\r\n",
        "\r\n",
        "* Representing text as numbers \r\n",
        "\r\n",
        "Why??\r\n",
        "\r\n",
        "* Machine learning model takes vectors ( arrays of numbers ) as input. \r\n",
        "* Using a better way to represent syntax and semantics of the underlying text being represented.\r\n",
        "\r\n",
        "How??\r\n",
        "\r\n",
        "Three strategies :\r\n",
        "\r\n",
        "1. one-hot encoding : Representing vocab (unique words) as columns and each word sequence as rows. The presence of words represented with 1 and absence as 0.\r\n",
        "\r\n",
        "  Eg. sentence :  \"The cat sat on the mat\"\r\n",
        "                        vocab      \r\n",
        "                  cat mat on sat the\r\n",
        "            the => 0   0   0  0   1\r\n",
        "            cat => 1   0   0  0   0\r\n",
        "            sat => 0   0   0  1   0\r\n",
        "            ...         ...\r\n",
        "\r\n",
        "      Combine the vectors of each word to form the encoding for the sentence \r\n",
        "\r\n",
        "      Disadvantage: \r\n",
        "      \r\n",
        "        Sparse (mostly 0) vector representation of words\r\n",
        "\r\n",
        "\r\n",
        "2.  Encode each word with a unique number :  Encode each word with unique number. Dence representation when compared to one-hot encoding. \r\n",
        "  \r\n",
        "        Eg. \r\n",
        "            sentence :  \"The cat sat on the mat\"\r\n",
        "\r\n",
        "              vector :   [5,  1,  4, 3,  5,  2]\r\n",
        "\r\n",
        "          \"the\" => 5\r\n",
        "        \r\n",
        "          \"cat\" => 1\r\n",
        "\r\n",
        "          ...\r\n",
        "\r\n",
        "  Disadvantage: \r\n",
        "        \r\n",
        "        1. Integer-encoding is arbitrary (Relationship between words not captured)\r\n",
        "        2. Not trainable \r\n",
        "\r\n",
        "3. Word embeddings: \r\n",
        "\r\n",
        "  * A word embedding is a trainable dense vector representation of floating point values. \r\n",
        "  * Values for word embedding are learned during training.\r\n",
        "  * Dimentionality or length of the vector is a hyper-parameter.\r\n",
        "  * Similar words have a similar encoding in word embedding wherein the semantic and syntactic relations are attended. \r\n",
        "\r\n",
        "          Eg. Analogies :  vec(“king”) - vec(“man”) + vec(“woman”) = vec(queen)\r\n",
        "Eg. sentence :  \"The cat sat on the mat\"\r\n",
        "                              vocab      \r\n",
        "                           cat    mat     on      sat      the\r\n",
        "                  the =>   0.5    0.56  -0.23    0.122    -0.111\r\n",
        "                  ...         ...\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OFydIJqolUs"
      },
      "source": [
        "#### Non-contextual word embeddings\r\n",
        "\r\n",
        "Word2Vec : http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK9ITwEu45qn"
      },
      "source": [
        "Pre- trained non - contextual word representation :  \r\n",
        "\r\n",
        "\"Word embeddings (word2vec, GloVe) are often\r\n",
        "pre-trained on text corpus from co-occurrence\r\n",
        "statistics\".\r\n",
        "\r\n",
        "* Word2Vec ( Window - based model ) :\r\n",
        "\r\n",
        "  Technique of training : \r\n",
        "    1.   Continuous Bag-of-words (CBOW) : Predict center word in the window based on surrounding words ( context before and after).\r\n",
        "    2.   Skip-gram (SG) : Predict surrounding words based on the central word in the window.\r\n",
        "\r\n",
        "  Disadvantages :\r\n",
        "    1. Window-based model, doesnot benefit from the information in the whole document.\r\n",
        "    2. Cannot handle OutOfVocabulary words (OOV) \r\n",
        "\r\n",
        "* Glove (Global Vectors for Word Representation) :\r\n",
        "  \r\n",
        "  * Construct matrix of (word x context) in a large corpus.\r\n",
        "  * Takes ratios of co-occurence probabilities.\r\n",
        "  * The embeddings are optimized , so that the dot product of 2 vectors equals the log of number of times the 2 words will occur near each other.\r\n",
        "\r\n",
        "  Disadvantages :\r\n",
        "  \r\n",
        "  1. Cannot handle OutOfVocabulary words (OOV) \r\n",
        "\r\n",
        "* Fasttext ( Fast and efficient representation ):\r\n",
        "\r\n",
        "  * Relies on skip-gram (SG) model.\r\n",
        "  * Benefits from sub-word information. \r\n",
        "  * Can handle OutOfVocabulary words (OOV).\r\n",
        "\r\n",
        "**Dis-advantage**  :\r\n",
        "\r\n",
        "* Static word embedding fail to capture polysemy (Same embedding for different contexts).\r\n",
        "\r\n",
        "eg. \"bank\"\r\n",
        "\r\n",
        "*  “I deposited 100 EUR in the **bank**.”\r\n",
        "* “She was enjoying the sunset o the left **bank** of the river.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8oAcLPVm9F0"
      },
      "source": [
        "### Language model\r\n",
        "\r\n",
        "LM : http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture06-rnnlm.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM0wEwDW8CJ7"
      },
      "source": [
        "What ??\r\n",
        "\r\n",
        "* A language model takes list of words and attempt to predict the probability of next word over the vocab ( unique words learned ) and *outputs the word with highest probability*.\r\n",
        "\r\n",
        "  * Computes probability distribution of next word (xt+1) over vocab V based on sequence x1,x2,....,xt.\r\n",
        "        p(xt,xt+1) = p(xt+1|xt,.....,x1)\r\n",
        "\r\n",
        "  * \"You can also think of a Language Model as a system that\r\n",
        "assigns probability to a piece of text.\"\r\n",
        "      \r\n",
        "        p(x1....xt) = product [p(xt | xt-1.....x1)]\r\n",
        "\r\n",
        "Why ??\r\n",
        "\r\n",
        "* Helps understanding language; with NLP application such as text generation (speech assistance), recognition, machine traslation. \r\n",
        "\r\n",
        "How ??\r\n",
        "\r\n",
        "* Basic architecture :\r\n",
        "\r\n",
        "  1. Look up embeddings \r\n",
        "  2. Calculate prediction with model\r\n",
        "  3. Project to output vocabulary and display words with highest probability.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9wLbhHJmWdH"
      },
      "source": [
        "**Language models Training** :\r\n",
        "\r\n",
        "**Idea** : \r\n",
        "\r\n",
        "1. Get text data  \r\n",
        "2. Select window size (history,word to be predicted) to slide against the text to generate dataset. \r\n",
        "3. Train model to generate next word Using history sequence as input. \r\n",
        "\r\n",
        "**Models for training LM** :\r\n",
        "\r\n",
        "1. Traditional n - gram language models : \r\n",
        "\r\n",
        "    * Predict n words (n-grams) based on the previous history of words( n-1 words ).\r\n",
        "    * \"The intuition of the n-gram model is that instead of computing the probability of a word given its entire history, we can approximate the history by just the last few.\"\r\n",
        "\r\n",
        "      Markov Assumption  : Probability of word depends on the previous word rather than the entire sequence of history.\r\n",
        "\r\n",
        "            Probability of next word given history :\r\n",
        "            bi-gram history = 2-1 = 1 ( only previous word)\r\n",
        "            P(next_word | history ) =  count( history, next_word) / count (history)\r\n",
        "  Problem :\r\n",
        "  * Sparsity Problems with n-gram Language Models ( counts could be 0)\r\n",
        "  * Storage of all n-grams\r\n",
        "  * Increasing n worsens sparsity problem, and increases model size\r\n",
        "\r\n",
        "2. Nueral language models : \r\n",
        "    * Deep neural networks models trained on corpus  which are better at  handling long - diatance relationships. \r\n",
        "\r\n",
        "  Training :\r\n",
        "\r\n",
        "    1. Get big corpus of text.\r\n",
        "    2. Feed into RNN-LM\r\n",
        "    3. Compute distribution of word  y^(t) over vocab for every step t.\r\n",
        "    4. Loss function(t) = cross-entropy  *summation of loss between the true label and log of predicted prob ( softmax prob)*  ( predicted prob, true prob )\r\n",
        "    \r\n",
        "  Types:\r\n",
        "\r\n",
        "    1. Fixed Window-based neural model\r\n",
        "        \r\n",
        "            Pipeline : Words/one-hot vectors (x) -> concatenated word embeddings (e) -> hidden layer  (h= f(We+b1)) -> output distribution over unique words (y = softmax( Uh +b2)\r\n",
        "\r\n",
        "          Dis-advantages :\r\n",
        "\r\n",
        "          1. Fixed window **too small**\r\n",
        "          2. Input words are multiplied by completely different weights in W. **No symmetry**  \r\n",
        "\r\n",
        "    2. RNN Language models\r\n",
        "    \r\n",
        "            Pipeline : Words/one-hot vectors (x) -> concatenated word embeddings (e) -> hidden layer  (h= sigmoid(Wh(t-1)+we(t)+b1)) -> output distribution over unique words (y = softmax( Uh +b2)\r\n",
        "\r\n",
        "        Dis-advantages: \r\n",
        "\r\n",
        "          1. Recurrent computation is slow and expencive. \r\n",
        "          2. Difficult to access information from many steps back. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhkth_EfofzM"
      },
      "source": [
        "#### Contextual word embeddings\r\n",
        "\r\n",
        "Source: https://www.slideshare.net/shuntaroy/a-review-of-deep-contextualized-word-representations-peters-2018\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5w38Z4nso3j"
      },
      "source": [
        "Pre- trained contextual word representation : Capturing word senses of language. \r\n",
        "\r\n",
        "*   ELMO : Embeddings from Language Models\r\n",
        "    * Learn word token vectors using long contexts ( whole sentence or longer) not context windows. \r\n",
        "    * Train **Seperate** Left-to-Right and Right-to-Left language models ( task agnostic and unsupervised ) using bi-LSTM .\r\n",
        "    * Use the task agnostic hidden states generated by the intermediate layers as \"Pre-trained contextualised embeddings\" . (every time step produces context specific word representation)\r\n",
        "      \r\n",
        "          word_embedding (hidden state) = previous_hidden_state which carry information + input_word_embedding*\r\n",
        "    \r\n",
        "    Steps:\r\n",
        "\r\n",
        "      * Concatenate hidden layers. \r\n",
        "      * Multiply each vector by a weight based on a task ( for scaling over the donwstream task).\r\n",
        "      * Sum the vectors.\r\n",
        "\r\n",
        "*   Flair\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gqH5bh3nWyn"
      },
      "source": [
        "#### Transfer learning \r\n",
        "\r\n",
        "Language Models pre-training : https://arxiv.org/pdf/1906.08237.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBCLdxUY46wS"
      },
      "source": [
        "* Generate language model using semi-supervised sequence learning trained over large text (unsupervised) rather than task specific to capture the patterns in the language.\r\n",
        "\r\n",
        "* **Using the pre-trained language model; finetune it for the particular downstream tasks** ( classification ).\r\n",
        "\r\n",
        "Idea (fast.ai) : \r\n",
        "\r\n",
        "1. In a pre-trained Nueral network (resnet34 cnn) first layers are more generic and last layers are more task specific. \r\n",
        "2. The final layer of pretrained model (which projects the output from the previous layers to the categories) is to be deleted and a new layer is added with some random weights for the downstream task.      \r\n",
        "3. **The previous layers are freezed from weight update during the SGD except the final added layer**.\r\n",
        "4. For NLP :\r\n",
        "  1. Freeze all layers except the last classifier layer ( randomly initialized weights) and train for few epochs.\r\n",
        "  2. Unfreeze all the layers and again train for few epochs with learning rate gradually increasing.\r\n",
        "          learn.fit_one_cycle(2, max_le = slice(1e-6,1e-4))\r\n",
        "\r\n",
        "\r\n",
        "Pre-training LM:\r\n",
        "\r\n",
        "1. Autoregressive LM : Factorize ( product of several factors ) the likelihood of word **after** a sequence (forward product of probabilities ) or word **before** a sequence ( backward product of probabilities )\r\n",
        "\r\n",
        "2. Autoencoding LM : Reconstruct the original sequence of text from the corrupted input (masked)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ2i6cyRn0vd"
      },
      "source": [
        "### OpenAI Transformer GPT : Pre - training a transformer Decoder for language modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLA_55IaPmRD"
      },
      "source": [
        "\"Improving Language Understanding by Generative\r\n",
        "Pre-Training, OpenAI, 2018\"\r\n",
        "*   Using Transformer decoder ( masked self attention ; no peeking at future tokens ) for the task of language modeling  as transformers are better choice when training on unlabeled large text data.\r\n",
        "*   12 - decoders are stacked from the vanilla transformer without the encoder-decoder attentention sublayer ( as encoder is absent).  \r\n",
        "* Fine-tune on downstream task (classification task).\r\n",
        "* Transformer decoders can be trained to generate text ( output for each timestep ).\r\n",
        "\r\n",
        "Scaled Version of GPT :\r\n",
        "\r\n",
        "1. GPT - 2 ( trained on 40B tokens of text )\r\n",
        "2. GPT - 3 ( trained on 300B tokens of text )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAxW7zBlSSMt"
      },
      "source": [
        "### Problem with autoregressive language models\r\n",
        "\r\n",
        "Source : https://web.stanford.edu/class/cs224n/slides/Jacob_Devlin_BERT.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiitMZ3VSXlk"
      },
      "source": [
        "* Language models only use left context or right context, but language understanding is bi-directional. \r\n",
        "\r\n",
        "* OpenAI tranformer LM is a left context / forward language model.\r\n",
        "\r\n",
        "Why??\r\n",
        "\r\n",
        "* Directionality is needed to generate a well-formed probability distribution.\r\n",
        "* Words can “see themselves” in a bidirectional encoder.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lye7fe-On9rK"
      },
      "source": [
        "### BERT : From Decoders to Encoders \r\n",
        "\r\n",
        "Source : https://web.stanford.edu/class/cs224n/slides/Jacob_Devlin_BERT.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZd3Jfwho5lK"
      },
      "source": [
        "* Encoders self attention spans the left and right tokens in the sequence. \r\n",
        "* Language models need to learn to predict future tokens which would not be possible using the encoders self attention which peeks into future tokens.\r\n",
        "\r\n",
        "Solution : Masked Language models ( autoencoding LM) \r\n",
        "\r\n",
        "* Mask out k% of the input words, and then predict the masked words. (k = 15%)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ats3rQUKju4g"
      },
      "source": [
        "BERT (Bidirectional Encoder Representations from Transformers):\r\n",
        "\r\n",
        "Pre-training of Deep Bidirectional Transformers for Language\r\n",
        "Understanding, which is then fine-tuned for a task.\r\n",
        "\r\n",
        "* Uses transformers **encoders** and **masked language modeling** to capture left and right context and handle long distance context. \r\n",
        "* Pretrained on Wikipedia + bookcorpus\r\n",
        "* Two varients based on pre-trained model sizes:\r\n",
        "  1. BERT-Base : 12 layers, 768-hidden, 12-heads\r\n",
        "  2. BERT-Large : 24 layers, 1024-hidden, 16-heads\r\n",
        "\r\n",
        "BERT Input Representation :\r\n",
        "\r\n",
        "* 30,000 wordpiece vocabulary on input.\r\n",
        "* Each token is sum of three embeddings.\r\n",
        "\r\n",
        "    Input -> Token embeddings ( represent word pieces ) + Segment embeddings ( represent segments of sentence pair ) + Positional embedding ( represents positions of word piece ) \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "BERT model fine tuning :\r\n",
        "\r\n",
        "* Use a classifier built on top layer for each task.\r\n",
        "* BERT trained for:\r\n",
        "  1. Sentence pair classification task. ( eg. Multi-NLI premise,hypothesis, label )\r\n",
        "  2. Single sentence classification tasks. (eg. CoLa - corpus of linguistic acceptability)\r\n",
        "  3. Question Answering task.\r\n",
        "  4. Single sentence tagging task.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKp5J86O28CN"
      },
      "source": [
        "### Post-BERT Pre-training Advancements\r\n",
        "\r\n",
        "Source : https://web.stanford.edu/class/cs224n/slides/Jacob_Devlin_BERT.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxd8YAD93C5h"
      },
      "source": [
        "1. RoBERTA : A Robustly Optimized BERT Pretraining\r\n",
        "Approach  (Liu et al, University of Washington and\r\n",
        "Facebook, 2019)\r\n",
        "  * Trained BERT for more epochs and on more data.\r\n",
        "2. XLNet : Generalized Autoregressive Pretraining for\r\n",
        "Language Understanding g (Yang et al, CMU and\r\n",
        "Google, 2019)\r\n",
        "  * Combination of auto-regressive (left-context) and auto-encoding (both context)\r\n",
        "  * Innovation #1 : Permutation LM \r\n",
        "      * Randomly permute the order of text rather than (left-to-right or       right-to-left) while training.\r\n",
        "\r\n",
        "  * Innovation #2 : Masked two stream attention (Relative position embeddings) \r\n",
        "    * Architecture change to handle permutation LM where two hidden representation for each token (h - Can attend every token including itself, g - Can peek into left context)\r\n",
        "3. ALBERT : A Lite BERT for Self-supervised Learning\r\n",
        "of Language Representations (Lan et al, Google\r\n",
        "and TTI Chicago, 2019)\r\n",
        "    * Innovation #1 : Factorized embedding\r\n",
        "parameterization\r\n",
        "      * Use small embedding size (e.g., 128) and then project it to\r\n",
        "Transformer hidden size (e.g., 1024) with parameter matrix\r\n",
        "\r\n",
        "  * Innovation #2: Cross-layer parameter sharing\r\n",
        "    *  Share all parameters between Transformer layers\r\n",
        "4. T5 \r\n",
        "  * Exploring the Limits of Transfer Learning with a\r\n",
        "Unified Text-to-Text Transformer (Raffel et al,\r\n",
        "Google, 2019)\r\n",
        "5. ELECTRA : Pre-training Text Encoders as\r\n",
        "Discriminators Rather Than Generators (Clark et al,\r\n",
        "2020)\r\n",
        "  * Train model to discriminate locally plausible text\r\n",
        "from real text\r\n",
        "\r\n",
        "Distillation :\r\n",
        "\r\n",
        "* BERT and other pre-trained language models are\r\n",
        "extremely large and expensive.\r\n",
        "* Train a distilled version of BERT (distillBERT) for smaller models.\r\n",
        "  * Technique : \r\n",
        "    1. Train \"Teacher\": Use SOTA( state of the art) + fine-tuning technique to train model with large data and maximum accuracy.\r\n",
        "    2. Train \"Student\" : Much smaller that mimics Teacher output.\r\n",
        "    3. Student objective is typically Mean Square Error or Cross Entropy\r\n",
        "    4. **Distillation works much better than pre-training +\r\n",
        "fine-tuning with smaller model**\r\n",
        "      why??\r\n",
        "        * Language modeling is the “ultimate” NLP task in many ways\r\n",
        "          * I.e., a perfect language model is also a perfect question\r\n",
        "            answering/entailment/sentiment analysis model\r\n",
        "        * Finetuning mostly just picks up and tweaks these existing latent\r\n",
        "features.\r\n",
        "        * Distillation allows the model to only focus on those features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQbbIuHz_oWA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}